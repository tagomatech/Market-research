{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Argus‑style Possibility Curves for MATIF Rapeseed\n",
        "\n",
        "**In‑sample exploration notebook** – we build and evaluate a 4‑parameter skew‑t model that forecasts the *full probability distribution* of the front‑month MATIF rapeseed future.\n",
        "\n",
        "### Sections\n",
        "1. Conceptual primer: APCs and the skew‑t distribution  \n",
        "2. Data preparation  \n",
        "3. Model definition (μ, σ, ν, τ)  \n",
        "4. Training loop (negative log‑likelihood)  \n",
        "5. Diagnostics: loss, quantile fan, PIT  \n",
        "6. Next steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generalised Distribution & Link–Function Architecture\n",
        "\n",
        "1. **Distribution**  \n",
        "\\[\\displaystyle\n",
        "  Y_{\\,t}^{\\text{indep.}} \\;\\sim\\; \\mathcal D\\!\\bigl(\\mu,\\;\\sigma,\\;\\nu,\\;\\tau\\bigr)\n",
        "\\]\n",
        "\n",
        "2. **Distribution parameters** — each obtained via a dedicated *link function* $g_i(\\cdot)$:\n",
        "\n",
        "\\[\\begin{aligned}\n",
        "\\eta_{1} &= g_{1}\\!\\bigl(\\mu\\bigr)  &&=\\; \\text{Fundamentals / Flows / Financials **or** forward‑curve levels} \\\\\n",
        "\\eta_{2} &= g_{2}\\!\\bigl(\\sigma\\bigr) &&=\\; \\text{Degree of uncertainty driven by Fundamentals / Flows / Financials} \\\\\n",
        "\\eta_{3} &= g_{3}\\!\\bigl(\\nu\\bigr)   &&=\\; \\text{Balance of risk (up‑ vs. down‑side) driven by Fundamentals / Flows / Financials} \\\\\n",
        "\\eta_{4} &= g_{4}\\!\\bigl(\\tau\\bigr)  &&=\\; \\text{Tail‑thickness (kurtosis) likewise driven by Fundamentals / Flows / Financials}\n",
        "\\end{aligned}\\]\n",
        "\n",
        "3. **Link‑function layer** — maps the chosen driver universe into the four distribution parameters, enabling separate functional forms for each moment of the predictive distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 Conceptual primer\n",
        "\n",
        "**Possibility Curve (APC)**: a daily cumulative distribution function $F_{t+1}(x)$ giving the probability that tomorrow’s price ≤ $x$.\n",
        "\n",
        "We model\n",
        "$$P_{t+1} \\sim \\text{Skew}\\,t\\big(\\mu_t,\\,\\sigma_t,\\,\\nu_t,\\,\\tau_t\\big)$$\n",
        "where\n",
        "* $\\mu$ location (expected level)  \n",
        "* $\\sigma$ scale (volatility)  \n",
        "* $\\nu$ skewness (asymmetry)  \n",
        "* $\\tau$ tail‑weight (kurtosis).\n",
        "\n",
        "We implement a **SinhArcsinh‑transformed Student‑$t$** (Fernández & Steel) for analytic CDF/quantile evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ta # Ensure 'ta' library is installed: pip install ta\n",
        "\n",
        "tfd, tfb = tfp.distributions, tfb.bijectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing and Feature Engineering Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_target(df: pd.DataFrame, price_col: str = \"rolled_close\", use_returns: bool = False) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Returns the modeling target: price or log-returns.\n",
        "    \"\"\"\n",
        "    if use_returns:\n",
        "        return np.log(df[price_col]).diff().dropna()\n",
        "    else:\n",
        "        return df[price_col]\n",
        "\n",
        "def compute_technical_features(df: pd.DataFrame, price_col: str = \"rolled_close\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add basic technical indicators using the 'ta' package.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # SMA\n",
        "    df[\"sma_20\"] = ta.trend.sma_indicator(df[price_col], window=20)\n",
        "    df[\"sma_50\"] = ta.trend.sma_indicator(df[price_col], window=50)\n",
        "\n",
        "    # RSI\n",
        "    df[\"rsi_14\"] = ta.momentum.rsi(df[price_col], window=14)\n",
        "\n",
        "    # ATR (volatility proxy)\n",
        "    # Note: For ATR, 'high' and 'low' typically refer to daily high/low. \n",
        "    # Here, we're using 'price_col' for all three, which assumes price_col is representative of daily range.\n",
        "    df[\"atr_14\"] = ta.volatility.average_true_range(high=df[price_col], low=df[price_col], close=df[price_col], window=14)\n",
        "\n",
        "    # Bollinger Band %B\n",
        "    bb = ta.volatility.BollingerBands(close=df[price_col], window=20, window_dev=2)\n",
        "    df[\"bb_pct\"] = bb.bollinger_pband()\n",
        "\n",
        "    # Z-score\n",
        "    mean = df[price_col].rolling(20).mean()\n",
        "    std = df[price_col].rolling(20).std()\n",
        "    df[\"z_score\"] = (df[price_col] - mean) / std\n",
        "\n",
        "    return df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Start of Data Loading and Feature Engineering ---\n",
        "# Load your actual data from a CSV file.\n",
        "# The file 'matif_osr_futs.csv' is expected to contain historical MATIF OSR futures data.\n",
        "# It should have at least the following columns:\n",
        "# - 'date': The date of the observation (e.g., '2022-01-03'). It will be converted to datetime objects.\n",
        "# - 'rolled_close': The rolled closing price of the futures contract.\n",
        "# The data represents the concatenation of the first line of Matif OSR futures that are rolled backward.\n",
        "\n",
        "try:\n",
        "    raw_df = pd.read_csv(\"matif_osr_futs.csv\")\n",
        "    raw_df[\"date\"] = pd.to_datetime(raw_df[\"date\"])\n",
        "    raw_df = raw_df.set_index(\"date\").sort_index()\n",
        "    print(\"Data loaded successfully. Sample of raw_df:\")\n",
        "    print(raw_df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'matif_osr_futs.csv' not found. Please ensure the file is in the correct directory.\")\n",
        "    print(\"Creating dummy data for demonstration purposes. Replace this with your actual data loading.\")\n",
        "    # Fallback to dummy data if file not found, for demonstration purposes\n",
        "    np.random.seed(42) # for reproducibility of dummy data\n",
        "    n_samples = 1000\n",
        "    initial_price = 500.0\n",
        "    price_series = initial_price + np.cumsum(np.random.normal(0, 5, n_samples))\n",
        "    dates = pd.date_range(start='2022-01-03', periods=n_samples, freq='D')\n",
        "    raw_df = pd.DataFrame({\n",
        "        \"date\": dates,\n",
        "        \"rolled_close\": price_series\n",
        "    })\n",
        "    raw_df[\"date\"] = pd.to_datetime(raw_df[\"date\"])\n",
        "    raw_df = raw_df.set_index(\"date\").sort_index()\n",
        "    print(raw_df.head())\n",
        "\n",
        "# Apply feature engineering\n",
        "# Ensure 'rolled_close' column exists before proceeding\n",
        "if \"rolled_close\" in raw_df.columns:\n",
        "    df_4_params = compute_technical_features(raw_df, price_col=\"rolled_close\")\n",
        "    print(\"\\nDataFrame after feature engineering (df_4_params) created with shape:\", df_4_params.shape)\n",
        "    print(df_4_params.head())\n",
        "else:\n",
        "    print(\"Error: 'rolled_close' column not found in the loaded data. Cannot proceed with feature engineering.\")\n",
        "    # Exit or handle error appropriately, e.g., by creating dummy df_4_params\n",
        "    # For now, setting df_4_params to an empty DataFrame to avoid further errors\n",
        "    df_4_params = pd.DataFrame()\n",
        "\n",
        "# --- End of Data Loading and Feature Engineering ---\n",
        "\n",
        "# Proceed only if df_4_params is not empty after feature engineering\n",
        "if not df_4_params.empty:\n",
        "    # target and features\n",
        "    y_raw = prepare_target(df_4_params, price_col=\"rolled_close\", use_returns=False).astype(\"float32\").to_numpy()\n",
        "    X_raw = df_4_params[[\"sma_20\",\"sma_50\",\"rsi_14\",\"atr_14\",\"bb_pct\",\"z_score\"]].astype(\"float32\").to_numpy()\n",
        "\n",
        "    # standardise target\n",
        "    y_mean, y_std = y_raw.mean(), y_raw.std()\n",
        "    y_z = ((y_raw - y_mean) / y_std).reshape(-1,1)\n",
        "\n",
        "    # normalise inputs with a Keras layer so scaling is saved in the model graph\n",
        "    norm = tf.keras.layers.Normalization(); norm.adapt(X_raw)\n",
        "else:\n",
        "    print(\"Skipping target and feature preparation due to empty df_4_params.\")\n",
        "    # Initialize y_raw, X_raw, y_z, norm to empty or default values to avoid errors downstream\n",
        "    y_raw = np.array([])\n",
        "    X_raw = np.array([])\n",
        "    y_z = np.array([])\n",
        "    norm = tf.keras.layers.Normalization()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 Skew‑t network\n",
        "* log‑σ and log‑τ ensure positivity  \n",
        "* tanh·5 caps $|\\nu|$ for numerical safety"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class APCSkewT(tf.keras.Model):\n",
        "    def __init__(self, hidden=32):\n",
        "        super().__init__()\n",
        "        # The normalization layer is passed from the outside, adapted to X_raw\n",
        "        self.norm = norm \n",
        "        self.f = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(hidden, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(hidden, activation=\"relu\")\n",
        "        ])\n",
        "        # Output layers for the four parameters\n",
        "        self.mu = tf.keras.layers.Dense(1, name='mu_output')\n",
        "        # log_s for scale (sigma), ensuring positivity with tf.exp\n",
        "        self.log_s = tf.keras.layers.Dense(1, bias_initializer=tf.constant_initializer(np.log(1.)), name='log_sigma_output')\n",
        "        # skew_h for skewness, capped by 5.*tf.tanh\n",
        "        self.skew_h = tf.keras.layers.Dense(1, name='skew_output')\n",
        "        # log_t for tailweight (tau), ensuring positivity with tf.exp\n",
        "        self.log_t = tf.keras.layers.Dense(1, bias_initializer=tf.constant_initializer(np.log(1.)), name='log_tau_output')\n",
        "\n",
        "    def call(self, x):\n",
        "        # Apply normalization to input features\n",
        "        x = self.norm(x)\n",
        "        # Pass through the shared feature extraction layers\n",
        "        h = self.f(x)\n",
        "        # Compute and return the four distribution parameters\n",
        "        return (\n",
        "            self.mu(h),\n",
        "            tf.exp(self.log_s(h)), # sigma must be positive\n",
        "            5. * tf.tanh(self.skew_h(h)), # skewness capped between -5 and 5\n",
        "            tf.exp(self.log_t(h)) # tailweight (tau) must be positive\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def skew_t(mu, sigma, skew, tail):\n",
        "    # Define the base distribution, a standard StudentT\n",
        "    # df=3. is a common choice for heavy-tailed distributions\n",
        "    base = tfd.StudentT(df=3., loc=0., scale=1.)\n",
        "    \n",
        "    # Apply transformations using bijectors:\n",
        "    # 1. SinhArcsinh: introduces skewness and tailweight\n",
        "    # 2. Scale: adjusts the spread (sigma)\n",
        "    # 3. Shift: adjusts the location (mu)\n",
        "    return tfd.TransformedDistribution(base, tfb.Chain([\n",
        "        tfb.Shift(mu),\n",
        "        tfb.Scale(sigma),\n",
        "        tfb.SinhArcsinh(skewness=skew, tailweight=tail)\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4 Training (negative log‑likelihood)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only run training if data is available\n",
        "if X_raw.size > 0 and y_z.size > 0:\n",
        "    # Create a TensorFlow Dataset for efficient batching and shuffling\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X_raw, y_z)).shuffle(500).batch(64)\n",
        "\n",
        "    # Initialize the model and optimizer\n",
        "    model = APCSkewT()\n",
        "    opt = tf.keras.optimizers.Adam(1e-3)\n",
        "\n",
        "    loss_log = []\n",
        "\n",
        "    @tf.function # Decorator to compile the function into a TensorFlow graph for performance\n",
        "    def step(bx, by):\n",
        "        with tf.GradientTape() as t:\n",
        "            # Get the predicted distribution parameters from the model\n",
        "            mu, s, k, tau = model(bx)\n",
        "            # Create the skew-t distribution\n",
        "            distribution = skew_t(mu, s, k, tau)\n",
        "            # Calculate the negative log-likelihood (NLL)\n",
        "            # tf.reduce_mean averages the NLL across the batch\n",
        "            loss = -tf.reduce_mean(distribution.log_prob(by))\n",
        "        \n",
        "        # Compute gradients and apply gradient clipping for stability\n",
        "        g, _ = tf.clip_by_global_norm(t.gradient(loss, model.trainable_variables), 2.)\n",
        "        # Apply gradients to update model weights\n",
        "        opt.apply_gradients(zip(g, model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    # Training loop\n",
        "    for ep in range(30):\n",
        "        # Compute average loss for the current epoch\n",
        "        ll = np.mean([step(bx, by).numpy() for bx, by in ds])\n",
        "        loss_log.append(ll)\n",
        "        print(f\"epoch {ep+1:02d}: NLL {ll:.4f}\")\n",
        "else:\n",
        "    print(\"Skipping training loop as no data is available.\")\n",
        "    loss_log = [] # Ensure loss_log is defined even if training is skipped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(loss_log, marker='o', linestyle='-', color='skyblue')\n",
        "plt.title(\"Training NLL Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"-log Likelihood\")\n",
        "plt.grid(True, linestyle='--', al
